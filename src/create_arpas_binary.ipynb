{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm running the code again for `create_arpas` but with new data! Creating binary models vs full grade levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from article_process_new import ArticleLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_data = '/Users/stephanie/data/newsela_articles_corpus_2019-03-25_SMALL'\n",
    "path_to_data = '/Users/stephanie/data/newsela_articles_corpus_2019-03-25'\n",
    "path_to_kenlm = '/Users/stephanie/github/kenlm'\n",
    "path_to_arpa = path_to_kenlm + '/lm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('/Users/stephanie/data/newsela_articles_corpus_2019-03-25/MANIFEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleLM = ArticleLM(path_to_data, path_to_kenlm, path_to_arpa, 5, 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleLM.metadata_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds the processed sentences that I need for KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleLM.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing easy\n",
      "Grade easy has 470591 sentences in the train set.\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 5549275 types 57394\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:688728 2:1340800512 3:2514001152 4:4022401792 5:5866002944\n",
      "Statistics:\n",
      "1 57394 D1=0.575252 D2=0.998303 D3+=1.60475\n",
      "2 861613 D1=0.703198 D2=1.15485 D3+=1.49267\n",
      "3 2340349 D1=0.805079 D2=1.27125 D3+=1.50709\n",
      "4 3308432 D1=0.873327 D2=1.41866 D3+=1.56887\n",
      "5 3582139 D1=0.7309 D2=1.67816 D3+=2.07964\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 211 assuming -p 1.5\n",
      "probing 249 assuming -r models -p 1.5\n",
      "trie     99 without quantization\n",
      "trie     53 assuming -q 8 -b 8 quantization \n",
      "trie     88 assuming -a 22 array pointer compression\n",
      "trie     42 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:688728 2:13785808 3:46806980 4:79402368 5:100299892\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:688728 2:13785808 3:46806980 4:79402368 5:100299892\n",
      "=== 5/5 Writing ARPA model ===\n",
      "RSSMax:2558857216 kB\tuser:7.2596\tsys:2.19241\tCPU:9.45203\treal:7.46036\n",
      "\n",
      "Processing hard\n",
      "Grade hard has 349293 sentences in the train set.\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "File stdin isn't normal.  Using slower read() instead of mmap().  No progress bar.\n",
      "Unigram tokens 6307200 types 76416\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:916992 2:1340778240 3:2513959424 4:4022334976 5:5865905152\n",
      "Statistics:\n",
      "1 76416 D1=0.592116 D2=0.99426 D3+=1.4465\n",
      "2 1091515 D1=0.720716 D2=1.15286 D3+=1.44777\n",
      "3 2804756 D1=0.822459 D2=1.3203 D3+=1.48931\n",
      "4 3799379 D1=0.88735 D2=1.533 D3+=1.57645\n",
      "5 4062187 D1=0.547836 D2=1.89867 D3+=2.44855\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 247 assuming -p 1.5\n",
      "probing 292 assuming -r models -p 1.5\n",
      "trie    118 without quantization\n",
      "trie     64 assuming -q 8 -b 8 quantization \n",
      "trie    105 assuming -a 22 array pointer compression\n",
      "trie     50 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:916992 2:17464240 3:56095120 4:91185096 5:113741236\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:916992 2:17464240 3:56095120 4:91185096 5:113741236\n",
      "=== 5/5 Writing ARPA model ===\n",
      "RSSMax:2570326016 kB\tuser:8.37908\tsys:2.44803\tCPU:10.8271\treal:8.05013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articleLM.train_all_arpas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out stats\n",
    "\n",
    "how many sentences there are in training set, indexed by grade level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy 470591\n",
      "hard 349293\n"
     ]
    }
   ],
   "source": [
    "for g in articleLM.level_sentences.get('train').keys():\n",
    "    print(g, len(articleLM.level_sentences.get('train').get(g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819884\n"
     ]
    }
   ],
   "source": [
    "# Number of sentences, train\n",
    "sentence_len = 0\n",
    "for g in articleLM.level_sentences.get('train').keys():\n",
    "    sentence_len+=len(articleLM.level_sentences.get('train').get(g))\n",
    "    \n",
    "print(sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102903\n"
     ]
    }
   ],
   "source": [
    "# Number of sentences, validation\n",
    "sentence_len = 0\n",
    "for g in articleLM.level_sentences.get('val').keys():\n",
    "    sentence_len+=len(articleLM.level_sentences.get('val').get(g))\n",
    "print(sentence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many articles there are in each set, indexed by grade level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    3910\n",
       "3    3286\n",
       "7    2883\n",
       "4    2560\n",
       "9    2028\n",
       "6    1839\n",
       "8    1592\n",
       "2    1029\n",
       "Name: grade_level, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articleLM.metadata_split[articleLM.metadata_split.train_val_test=='train'].grade_level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19127"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articleLM.metadata_split[articleLM.metadata_split.train_val_test=='train'].grade_level.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    507\n",
       "3    427\n",
       "7    346\n",
       "4    300\n",
       "6    244\n",
       "9    235\n",
       "8    188\n",
       "2    144\n",
       "Name: grade_level, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articleLM.metadata_split[articleLM.metadata_split.train_val_test=='val'].grade_level.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing perplexities - Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplex_guesses = articleLM.compute_all_sentences_best_guess(['easy', 'hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6743340816108374"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(perplex_guesses[perplex_guesses.level == perplex_guesses.best_guess] ) * 1. / len(perplex_guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>best_guess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>easy</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level best_guess\n",
       "0  easy       easy\n",
       "1  easy       easy\n",
       "2  easy       easy\n",
       "3  easy       easy\n",
       "4  easy       hard"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplex_guesses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_article_best_guess():\n",
    "    article_sentences = articleLM.article_val_sentences\n",
    "\n",
    "    iter = 0\n",
    "    article_df = pd.DataFrame({})\n",
    "    for article_gl in article_sentences.keys():\n",
    "        #     mean_guess, median_guess = articleLM.compute_perplexity_entire_article(article_sentences[article_gl])\n",
    "\n",
    "        sample_perp = dict()\n",
    "        for ix, sentence in enumerate(article_sentences[article_gl]):\n",
    "            perplexities = articleLM.compute_sentence_perplexities(sentence)\n",
    "            sample_perp[ix] = perplexities\n",
    "        sample_perp_df = pd.DataFrame(sample_perp).T\n",
    "        sample_perp_df.columns = sorted(articleLM.models.keys())\n",
    "\n",
    "        sample_perp_df.loc[:, 'min_perplexity'] = sample_perp_df.idxmin(axis=1, skipna=True)\n",
    "        sample_perp_df.loc[:, 'true_gl'] = article_gl[1]\n",
    "        sample_perp_df.loc[:, 'article'] = article_gl[0]\n",
    "\n",
    "        article_df = pd.concat([article_df, sample_perp_df])\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 10 == 0:\n",
    "            print(\"iteration {}, article {}\".format(iter, article_gl))\n",
    "\n",
    "    return article_df\n",
    "\n",
    "# def plot_best_article_guess():\n",
    "#     article_df = articleLM.compute_article_best_guess()\n",
    "#     means = article_df.groupby(['article', 'true_gl']).mean()['min_perplexity'].reset_index()\n",
    "#     means.rename(columns={'min_perplexity': 'predicted_gl'}, inplace=True)\n",
    "\n",
    "#     fig = plt.figure(figsize=(10, 8))\n",
    "#     ax = fig.gca()\n",
    "#     ax.set_title(\"Grade Level Distributions\")\n",
    "#     # ax.set_xlabel(\"True Grade Level\")\n",
    "#     # ax.set_ylabel(\"Predicted Grade Level\")\n",
    "#     sns.boxplot(ax=ax, x=\"true_gl\", y=\"predicted_gl\", data=means)\n",
    "#     ax.set_xlabel(\"True Grade Level\")\n",
    "#     ax.set_ylabel(\"Predicted Grade Level, using Means\")\n",
    "\n",
    "#     return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, article ('gerrymandering-redistricting', 'easy')\n",
      "iteration 20, article ('hawaii-philosophical', 'easy')\n",
      "iteration 30, article ('nobel-literature', 'easy')\n",
      "iteration 40, article ('mexico-earthquake', 'easy')\n",
      "iteration 50, article ('parents-let-teens-ride-uber', 'easy')\n",
      "iteration 60, article ('mummy-scans', 'easy')\n",
      "iteration 70, article ('tortoise-shells', 'hard')\n",
      "iteration 80, article ('teen-nasajob', 'easy')\n",
      "iteration 90, article ('tsunami-pod', 'easy')\n",
      "iteration 100, article ('netflix-fortnite', 'easy')\n",
      "iteration 110, article ('preslibrary-land', 'easy')\n",
      "iteration 120, article ('nutria-laststand', 'hard')\n",
      "iteration 130, article ('pokemon-augmented-reality', 'hard')\n",
      "iteration 140, article ('opinion-how-transform-education-system', 'easy')\n",
      "iteration 150, article ('teens-sleep', 'easy')\n",
      "iteration 160, article ('toyota-flying-car-and-luxury-boat', 'hard')\n",
      "iteration 170, article ('kids-hugging-holidays', 'easy')\n",
      "iteration 180, article ('mlb-steroids', 'easy')\n",
      "iteration 190, article ('sharks-hawaii', 'hard')\n",
      "iteration 200, article ('syria-palmyra', 'easy')\n",
      "iteration 210, article ('bosnia-swim-club', 'easy')\n",
      "iteration 220, article ('a-wrinkle-in-time-for-everyone', 'easy')\n",
      "iteration 230, article ('instagram-cultural-spaces', 'hard')\n",
      "iteration 240, article ('amphibian-decline', 'easy')\n",
      "iteration 250, article ('hearst-pool', 'easy')\n",
      "iteration 260, article ('turkey-riots', 'hard')\n",
      "iteration 270, article ('sorting-hat-spider', 'easy')\n",
      "iteration 280, article ('boston-olympicbid', 'easy')\n",
      "iteration 290, article ('natural-disasters-rome-volcano', 'easy')\n",
      "iteration 300, article ('ravens-termination', 'hard')\n",
      "iteration 310, article ('la-minwage', 'easy')\n",
      "iteration 320, article ('hoverboards-safetytesting', 'easy')\n",
      "iteration 330, article ('human-heart-art', 'easy')\n",
      "iteration 340, article ('mars-hershey-chocolate-war', 'easy')\n",
      "iteration 350, article ('civilrights-police', 'easy')\n",
      "iteration 360, article ('spelling-bee', 'easy')\n",
      "iteration 370, article ('medal-honor-2016', 'hard')\n",
      "iteration 380, article ('Braille-startup', 'hard')\n",
      "iteration 390, article ('students-drillteam', 'hard')\n",
      "iteration 400, article ('elem-sesame-street-theme-park-autism-certified', 'easy')\n",
      "iteration 410, article ('pizza-tossing-naples', 'easy')\n",
      "iteration 420, article ('canada-securitylaws', 'hard')\n",
      "iteration 430, article ('rooftop-farms', 'easy')\n",
      "iteration 440, article ('mars-finalist', 'easy')\n",
      "iteration 450, article ('gilgamesh-lines', 'easy')\n",
      "iteration 460, article ('old-toys-reinvention', 'hard')\n",
      "iteration 470, article ('opinion-end-high-school-football', 'easy')\n",
      "iteration 480, article ('happier-meal-mcdonalds', 'easy')\n",
      "iteration 490, article ('northkorea-statues', 'easy')\n",
      "iteration 500, article ('braincontrolled-drone', 'easy')\n",
      "iteration 510, article ('refugees-garden', 'hard')\n",
      "iteration 520, article ('student-art-ferguson-congressman', 'hard')\n",
      "iteration 530, article ('americanjewish-soldier', 'easy')\n",
      "iteration 540, article ('innovation-discontents', 'easy')\n",
      "iteration 550, article ('mars-toxic-chemicals', 'easy')\n",
      "iteration 560, article ('homework-burden', 'easy')\n",
      "iteration 570, article ('fantasysports-legality', 'hard')\n",
      "iteration 580, article ('warriors-win-nba-finals', 'hard')\n",
      "iteration 590, article ('goldfish-suit', 'hard')\n",
      "iteration 600, article ('white-rhino-sudan-dies', 'easy')\n",
      "iteration 610, article ('antarctica-marine-reserve', 'easy')\n",
      "iteration 620, article ('shark-bites', 'easy')\n",
      "iteration 630, article ('border-music', 'hard')\n",
      "iteration 640, article ('students-respond-to-hate-with-heart-stickers', 'hard')\n",
      "iteration 650, article ('flyers-mascot-gritty', 'easy')\n",
      "iteration 660, article ('sorting-hat-spider', 'hard')\n",
      "iteration 670, article ('tortoise-shells', 'easy')\n",
      "iteration 680, article ('polynesian-canoe', 'hard')\n",
      "iteration 690, article ('FCC-net-neutrality', 'hard')\n",
      "iteration 700, article ('caribbean-seaweed', 'hard')\n",
      "iteration 710, article ('polar-bears-alaskan-natives', 'easy')\n",
      "iteration 720, article ('childrens-movies-for-everyone', 'easy')\n",
      "iteration 730, article ('royal-budget', 'easy')\n",
      "iteration 740, article ('dare-dictionary', 'hard')\n",
      "iteration 750, article ('swedish-queen-lake-sword', 'easy')\n",
      "iteration 760, article ('syria-wheelchair-journey', 'hard')\n",
      "iteration 770, article ('japanese-whaling-culture', 'hard')\n",
      "iteration 780, article ('seattle-sleepystudents', 'hard')\n",
      "iteration 790, article ('students-3D-printing', 'easy')\n",
      "iteration 800, article ('UN-emissions-rules', 'easy')\n",
      "iteration 810, article ('headscarves-harassment', 'easy')\n",
      "iteration 820, article ('3D-printed-homes-shelter-vulnerable', 'hard')\n",
      "iteration 830, article ('asian-art', 'easy')\n",
      "iteration 840, article ('nfl-procon', 'hard')\n",
      "iteration 850, article ('first-flag-debate', 'hard')\n",
      "iteration 860, article ('immigration-el-salvador', 'hard')\n",
      "iteration 870, article ('dogs-elephantherding', 'hard')\n",
      "iteration 880, article ('government-shut-down-ends', 'easy')\n",
      "iteration 890, article ('african-famine-whatsapp', 'hard')\n",
      "iteration 900, article ('jellyfish-sleep', 'easy')\n",
      "iteration 910, article ('baby-chimp', 'hard')\n",
      "iteration 920, article ('marley-dias-1000blackgirlbooks', 'easy')\n",
      "iteration 930, article ('fish-legs-robot', 'easy')\n",
      "iteration 940, article ('malala-sanjose', 'hard')\n",
      "iteration 950, article ('diverse-barbie', 'hard')\n",
      "iteration 960, article ('china-statemedia', 'hard')\n",
      "iteration 970, article ('ant-rafts', 'hard')\n",
      "iteration 980, article ('lebron-james-local-economy-impact', 'hard')\n",
      "iteration 990, article ('halloween-sales', 'easy')\n",
      "iteration 1000, article ('fruits-genetics', 'hard')\n",
      "iteration 1010, article ('gaymarriage-scotus', 'easy')\n",
      "iteration 1020, article ('hackable-barbie', 'hard')\n",
      "iteration 1030, article ('barbie-stem', 'hard')\n",
      "iteration 1040, article ('Rohingya-refugee-camps-solar-panels', 'easy')\n",
      "iteration 1050, article ('uzi-kids', 'easy')\n",
      "iteration 1060, article ('ecuador-radiodrama', 'hard')\n",
      "iteration 1070, article ('orcas-exhale', 'easy')\n",
      "iteration 1080, article ('whitehouse-kids', 'hard')\n",
      "iteration 1090, article ('perfecting-space-cuisine', 'hard')\n",
      "iteration 1100, article ('un-report-biodiversity', 'easy')\n",
      "iteration 1110, article ('new-american-revolution-museum', 'hard')\n",
      "iteration 1120, article ('obama-tribe', 'easy')\n",
      "iteration 1130, article ('video-games-exercise', 'easy')\n",
      "iteration 1140, article ('tsunami-pod', 'hard')\n",
      "iteration 1150, article ('whale-ancient', 'hard')\n",
      "iteration 1160, article ('hurricane-forecasting', 'easy')\n",
      "iteration 1170, article ('karate-80yo', 'easy')\n",
      "iteration 1180, article ('extreme-treks-and-climbs', 'easy')\n",
      "iteration 1190, article ('columbus-day-pro-con', 'hard')\n",
      "iteration 1200, article ('nigeria-president', 'easy')\n",
      "iteration 1210, article ('jimmy-chin-art-of-chill', 'easy')\n",
      "iteration 1220, article ('teen-wants-compassionate-cure-cancer', 'easy')\n",
      "iteration 1230, article ('dumpster-fire-added-to-dictionary', 'easy')\n",
      "iteration 1240, article ('MLK-teens-march', 'hard')\n",
      "iteration 1250, article ('goats-happy-faces', 'hard')\n",
      "iteration 1260, article ('obama-lameduck', 'hard')\n",
      "iteration 1270, article ('iran-nuclear', 'easy')\n",
      "iteration 1280, article ('elem-goldfish-rescue-paris-aquarium', 'easy')\n",
      "iteration 1290, article ('deported-soccer-star', 'easy')\n",
      "iteration 1300, article ('flying-reptile-prehistoric', 'hard')\n",
      "iteration 1310, article ('procon-religion-school', 'hard')\n",
      "iteration 1320, article ('elem-labels-for-education-junk-food', 'easy')\n",
      "iteration 1330, article ('fake-news-ramifications', 'easy')\n",
      "iteration 1340, article ('venomous-animals-australia', 'hard')\n",
      "iteration 1350, article ('tiger-recording', 'hard')\n",
      "iteration 1360, article ('fisherman-aztec-floating-gardens', 'easy')\n",
      "iteration 1370, article ('what-causes-wildfires', 'hard')\n",
      "iteration 1380, article ('blade-runners-advantage', 'hard')\n",
      "iteration 1390, article ('vaquita-porpoise-protection', 'easy')\n",
      "iteration 1400, article ('sealions-braindamage', 'easy')\n",
      "iteration 1410, article ('detroitschools-sickout', 'hard')\n",
      "iteration 1420, article ('lester-holt-teens-fact-checking', 'easy')\n",
      "iteration 1430, article ('mummies-DNA', 'hard')\n",
      "iteration 1440, article ('olympics-skiing-virtual-reality', 'hard')\n",
      "iteration 1450, article ('france-suburbs', 'easy')\n",
      "iteration 1460, article ('states-recycling', 'hard')\n",
      "iteration 1470, article ('german-obituaries', 'easy')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1480, article ('crow-funerals', 'easy')\n",
      "iteration 1490, article ('video-games-kids-brains', 'hard')\n",
      "iteration 1500, article ('not-all-teenagers-drink', 'hard')\n",
      "iteration 1510, article ('end-of-world-records', 'easy')\n",
      "iteration 1520, article ('elem-ocean-hot-spots', 'easy')\n",
      "iteration 1530, article ('mars-ice-cap-water', 'easy')\n",
      "iteration 1540, article ('deep-sea-creature-discovery', 'hard')\n",
      "iteration 1550, article ('elem-part-time-hermit', 'easy')\n",
      "iteration 1560, article ('beasts-death-valley', 'hard')\n",
      "iteration 1570, article ('elem-dog-true-cost', 'easy')\n",
      "iteration 1580, article ('unique-giant-sloth-fossil', 'hard')\n",
      "iteration 1590, article ('religion-accommodation', 'hard')\n",
      "iteration 1600, article ('elem-manta-ray-nursery-discovery', 'easy')\n",
      "iteration 1610, article ('elem-hero-dogs-program', 'easy')\n",
      "iteration 1620, article ('fat-bear-week', 'easy')\n",
      "iteration 1630, article ('new-women-congress-fashion-statement', 'hard')\n",
      "iteration 1640, article ('fall-equinox', 'easy')\n",
      "iteration 1650, article ('wimpy-kids-meltdown-show', 'easy')\n",
      "iteration 1660, article ('trump-approval-keystone-pipeline', 'hard')\n",
      "iteration 1670, article ('newzealand-flag', 'easy')\n",
      "iteration 1680, article ('elem-crying-sumo-babies', 'easy')\n",
      "iteration 1690, article ('elem-oldest-sea-monster-fossil', 'easy')\n",
      "iteration 1700, article ('social-media-poets-go-viral', 'easy')\n",
      "iteration 1710, article ('whats-happening-in-venezuela', 'hard')\n",
      "iteration 1720, article ('spaghetti-breaking-experiment', 'hard')\n",
      "iteration 1730, article ('elem-new-monopoly-tokens', 'easy')\n",
      "iteration 1740, article ('elem-toy-hall-of-fame', 'easy')\n",
      "iteration 1750, article ('kilogram-redefined', 'easy')\n",
      "iteration 1760, article ('banksy-artwork-self-destructs', 'easy')\n",
      "iteration 1770, article ('amazon-toy-catalog', 'easy')\n",
      "iteration 1780, article ('royals-rhinopark', 'hard')\n",
      "iteration 1790, article ('elem-mice-settlements', 'easy')\n",
      "iteration 1800, article ('why-do-we-sleep', 'hard')\n",
      "iteration 1810, article ('artificial-wave', 'hard')\n",
      "iteration 1820, article ('book-explains-me-too-kids', 'easy')\n"
     ]
    }
   ],
   "source": [
    "most_common = articleLM.compute_article_best_guess_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(most_common.true_gl, most_common.predicted_gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[702, 338],\n",
       "       [126, 661]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746031746031746"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(most_common.true_gl, most_common.predicted_gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7516059957173448"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(most_common.true_gl, most_common.predicted_gl, pos_label='easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
